---
layout: post
categories: blog
---

# SIMD

链接：https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#expand=3904,3913,4011,34,4014,4602,4011

指令集分类如下

## 1.MMX

- **首次引入**：1997 年（Pentium MMX）。
- **寄存器**：8 个 64-bit 寄存器（`mm0`~`mm7`），与 x87 FPU 共享。
- **数据类型**：整数（8-bit, 16-bit, 32-bit）。
- **主要应用**：图像、视频处理。
- **局限性**：仅支持整数运算，不支持浮点运算；由于与 x87 FPU 共享寄存器，需要 `EMMS` 指令清理状态。



## 2.SSE

- **首次引入**：1999 年（Pentium III）。
- **寄存器**：8 个 128-bit 浮点寄存器（`xmm0`~`xmm7`），后扩展至 16 个（x86-64 模式）。
- **主要改进**：独立的 SIMD 浮点寄存器，不再与 FPU 共享；支持单精度浮点数（SSE2 开始支持整数运算）。

### SSE

发布于 1999，主要改进是 128-bit SIMD，支持单精度浮点运算。

### SSE2

发布于 2001，新增了 144 条指令，支持 **整数运算** + **双精度浮点**，主要用于科学计算、加密、视频处理。

### SSE3

发布于 2004，新增了 13 条指令，支持**水平加法**、**线程同步**，主要用于矩阵运算、DSP。

### SSSE3

发布于 2006，新增了 32 条指令，支持**整数混洗、绝对值、乘法加法**，主要用于图像处理、加密、压缩。

### SSE4.1

发布于 2007，新增了 47 条指令，支持**点积、插值、整数插入提取**，主要用于 3D 图形、物理仿真。

### SSE4.2

发布于 2008，新增了 7 条指令，支持**字符串处理、CRC32**，主要用于字符串匹配、网络协议校验。



## 3.AVX

- **首次引入**：2011 年（Sandy Bridge）。
- **寄存器**：16 个 256-bit `ymm0`~`ymm15`（x86-64）。
- **主要改进**：扩展寄存器宽度到 256-bit，提高吞吐量；支持 FMA（Fused Multiply-Add，融合乘加）。



### AVX

- **首次引入**：2011 年（Sandy Bridge）
- **主要特点**：256-bit 寄存器（`ymm0` ~ `ymm15`）；**支持 FMA（融合乘加），但 FMA 需要额外指令集支持**；**改进指令编码**（VEX 编码，减少指令长度）。
- **用途**：**浮点运算优化**（科学计算、金融计算）；**多媒体处理**；**加密算法**。



### F16C（Float 16 Conversion）

- **首次引入**：2012 年（Ivy Bridge）。
- **主要特点**：**支持 float32（单精度） 和 float16（半精度） 转换**；主要用于 **深度学习**，因为 float16 在 AI 计算中更节省内存。
- **用途**：**神经网络（AI 计算）**；**图形渲染**。



### FMA

Fused Multiply-Add

- **首次引入**：2013 年（Haswell）。

- **主要特点**：**融合乘加**（`a * b + c` 作为单个指令），提高计算效率；减少舍入误差，提高精度。
- **用途**：**科学计算**；**神经网络训练**；**物理仿真**。



### **AVX2**

- **首次引入**：2013 年（Haswell）。
- **主要特点**：**AVX 的整数扩展**；**支持 256-bit 整数运算**（SSE 只能 128-bit）；**支持 `gather` 指令**（从多个地址加载数据）；**更高效的 bit 操作**。
- **用途**：**图像处理**；**机器学习**；**高性能计算**。
- 

### AVX_VNNI

Vector Neural Network Instructions

- **首次引入**：2021 年（Alder Lake）。
- **主要特点**：主要用于 **加速 INT8 计算**（深度学习推理）；允许高效执行 **点积运算**；计算模式：`C += A * B`。
- **用途**：**神经网络推理**；**机器学习**。



### AVX_VNNI_INT8

- **首次引入**：2022 年（Sapphire Rapids）。
- **主要特点**：**针对 INT8 数据的 VNNI 扩展**；**提升 AI 计算效率**。
- **用途**：**加速 AI 推理**（INT8 计算）；**机器学习**；**边缘计算（低功耗 AI）**。



### AVX_NE_CONVERT

- **首次引入**：2022 年（Sapphire Rapids）。
- **主要特点**：**支持新的浮点格式转换**；可在 **BF16（Brain Floating Point）与 FP32 之间转换**。
- **用途**：**深度学习（FP32 → BF16 提高计算效率）**；**图像处理**。



### AVX_IFMA

Integer Fused Multiply-Add

- **首次引入**：2023 年（Meteor Lake）。
- **主要特点**：**用于大整数（Big Integer）计算**；**支持 52-bit 整数 FMA 计算**。
- **用途**：**密码学（RSA 加密）**；**高精度整数计算**。





### AVX_VNNI_INT16

- **首次引入**：2023 年（Meteor Lake）。
- **主要特点**：**扩展 AVX_VNNI 以支持 INT16**；**适用于高精度 AI 计算**。
- **用途**：**神经网络计算**；**AI 推理（更高精度）**。



### SHA512

- **首次引入**：2023 年（Meteor Lake）
- **主要特点**：**加速 SHA-512 哈希计算**。
- **用途**：**安全协议**；**加密**。



### SM3/SM4

- **首次引入**：2023 年（Meteor Lake）。
- **主要特点**：**中国国家标准密码算法**；**SM3：哈希算法（类似 SHA-256）**；**SM4：对称加密算法（类似 AES）**。
- **用途**：**政府和企业安全通信**；**加密存储**。



## 4.AVX-512

- **首次引入**：2016 年（Skylake-X，Xeon 服务器）。
- **寄存器**：16 个 512-bit `zmm0`~~`zmm15`，支持 32 个寄存器（`zmm0`~~`zmm31`）在特定架构中。
- **掩码寄存器**：增加 8 个 64-bit `k0`~`k7`，用于条件执行。
- **主要改进**：512-bit 超宽向量，提高吞吐量；支持 `VL`（Vector Length）灵活调整宽度（128/256/512-bit）；新增矩阵计算、神经网络优化指令。



## 5.AMX

- **首次引入**：Intel Sapphire Rapids（2023） 。
- **主要特点**：主要用于 **人工智能（AI）** 和 **矩阵计算**；不是传统的 SIMD，而是一个 **Tile-Based 加速** 方案；依赖 **新增加的 Tile 寄存器**（8×1024-bit）。
- **关键机制**：8 个 **Tile 寄存器（TMM0-TMM7）**，每个 1024-bit。

- **指令**：`AMX_TILELOAD`（从内存加载矩阵数据）；`AMX_TILEMUL`（矩阵乘法）；`AMX_TILEST`（存回结果）。
-  **应用**：**深度学习推理**（TensorFlow, PyTorch）；**图像/视频处理**；**大规模矩阵运算**。



## 6.SVLM

- SVLM**不是一个硬件指令集**，而是 Intel 提供的 **数学运算库**，用于支持 SIMD 计算。
- 主要用于 **三角函数、指数、对数等数学计算**。

- 适用于 **SSE、AVX、AVX-512** 平台。
- **为什么需要 SVML？**在 SIMD 中计算 `sin(x)`, `exp(x)`, `log(x)` 这些数学函数并不直接受 CPU 指令支持。**SVML 通过优化的向量数学库** 实现了这些计算，并且能够 **自动向量化**。



## 7.Other

需要则补充。



# 相比于 GPU

SIMD（单指令多数据）和GPU（图形处理单元）都用于加速计算，但它们的设计目标、适用场景以及局限性有所不同：

### 1. **架构区别**

| 特性             | SIMD                                | GPU                                  |
| ---------------- | ----------------------------------- | ------------------------------------ |
| **执行单元**     | CPU内部的向量寄存器和SIMD ALU       | 多个流处理器（CUDA核心、SM）         |
| **并行粒度**     | 指令级别（单指令作用于多个数据）    | 线程级别（多个线程并行执行）         |
| **控制逻辑**     | 受CPU控制，适合低延迟计算           | 适合大规模并行计算，控制逻辑相对简单 |
| **存储架构**     | 依赖CPU缓存层次结构（L1/L2/L3缓存） | 具有大量专用显存（GDDR、HBM）        |
| **适用编程模型** | AVX、SSE、Neon、Altivec等           | CUDA、OpenCL、Metal、DirectCompute等 |

### 2. **适用场景**

| 计算任务                           | SIMD                         | GPU                                  |
| ---------------------------------- | ---------------------------- | ------------------------------------ |
| **图像/视频处理**                  | 部分适用（如AVX512加速滤波） | 强适用（如CUDA/OpenCL加速图像处理）  |
| **科学计算（矩阵运算、线性代数）** | 适用于小规模矩阵运算         | 适用于大规模矩阵运算（如深度学习）   |
| **游戏/3D渲染**                    | 仅限CPU端优化                | GPU核心任务，极其适合                |
| **加密/哈希计算**                  | 适合特定指令集（AES-NI等）   | 适用于大规模哈希计算（如密码学挖矿） |
| **人工智能/深度学习**              | 仅适用于小规模推理           | 训练和推理都适用，Tensor Core优化    |
| **金融建模（如蒙特卡洛模拟）**     | 适用于小规模计算             | 适用于大规模并行计算                 |
| **物理仿真**                       | 适用于少量并行计算           | 适用于大规模粒子仿真                 |

### 3. **局限性**

#### **SIMD 局限性**

1. **并行度受限**：CPU SIMD单元通常只有128–512位宽（如SSE/AVX），并行度有限。
2. **数据对齐要求**：SIMD通常要求数据按对齐方式存储，否则会有性能损失。
3. **分支处理能力较弱**：SIMD流水线受限于分支预测，遇到复杂逻辑时效率下降。
4. **扩展性受限**：受CPU架构限制，SIMD无法轻松扩展至大规模并行计算。

#### **GPU 局限性**

1. **延迟较高**：GPU适用于高吞吐量但延迟较高的任务，不适合低延迟计算。
2. **指令灵活性较差**：GPU适合数据并行计算，不适合复杂的控制流。
3. **CPU-GPU通信开销**：数据传输（PCIe带宽限制）可能成为瓶颈，影响性能。
4. **依赖专门的软件优化**：需要CUDA、OpenCL等特定框架，不如SIMD易用。
